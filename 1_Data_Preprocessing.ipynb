{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation (MT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my project I created a Basic Machine Translation Model which includes the following main steps:\n",
    "- Preprocess data\n",
    "- Pretrained embeddings using Word2Vec\n",
    "- Generate machine translation using sequence-to-sequence (encoder-decoder) model with LSTM cells\n",
    "<br>\n",
    "\n",
    "#### Motivation for Machine Translation\n",
    "Machine Translation is the use of software programs which have been specifically designed to automatically translate text from one language to another, without human involvement. Especially in the time of rapid globalization, such services gain great importance and become invaluable in many application fields. It has some great advantages (compared with human translation):\n",
    "- **Speed**: Machine translation is fast, can quickly translate content and provide a quality output to the user in no time\n",
    "- **Cost**: Machine translation is cheap in comparison to employing a professional translator\n",
    "- **Confidentiality**: Machine translation protects sensitive information which might be risky to give to a professional translator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "The data I used for generating a machine translation model is **[European Parliament Proceedings Parallel Corpus 1996-2011](http://www.statmt.org/europarl/)** \n",
    "Therof I used the parallel corpus Spanish-English"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Datasets (en, es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dataset_path\n",
    "dataset_path = './Dataset_MT'\n",
    "if not os.path.exists(dataset_path):\n",
    "    os.makedirs(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for data loading\n",
    "def load_doc(filename):\n",
    "    # Open the file as read only\n",
    "    file = open(dataset_path + '/' + filename, mode='rt', encoding='utf-8')\n",
    "    # Read all text\n",
    "    text = file.read()\n",
    "    # Close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the loaded texts into datasets of sentences\n",
    "def to_sentences(doc):\n",
    "    return doc.strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the English dataset\n",
    "text_en = load_doc('europarl-v7.es-en.en')\n",
    "text_en = to_sentences(text_en)\n",
    "\n",
    "# Load the Spanish dataset\n",
    "text_es = load_doc('europarl-v7.es-en.es')\n",
    "text_es = to_sentences(text_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1965734 1965734\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the datasets\n",
    "print(len(text_en), len(text_es))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.', \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\", 'You have requested a debate on this subject in the course of the next few days, during this part-session.', \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\"]\n",
      "['Declaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.', 'Como todos han podido comprobar, el gran \"efecto del año 2000\" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.', 'Sus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.', 'A la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.']\n"
     ]
    }
   ],
   "source": [
    "# Observe the first samples of both datasets\n",
    "print(text_en[1:5])\n",
    "print(text_es[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consequences of observing the datasets**:<br>\n",
    "For the normalization process I regarded the following steps as necessary \n",
    "- Convert to lower case\n",
    "- Remove special characters (in Spanish dataset: Replace language specific letters by ascii letters e.g. Señorías --> Senorias)\n",
    "\n",
    "Although I cannot see a contraction in the samples I decided to add the expansion of possible contractions to the normalization process (to prevent the risk).<br>\n",
    "Additionally I decided to also add the option for lemmatization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Preprocessing Step: Defining functions for saving/loading intermediate results of preprocessing\n",
    "\n",
    "These functions are defined in advance to save and load intermediate results of the preprocessing process and thereby save time while working on the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from pickle import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define save_path\n",
    "save_path_for_data = './Preprocessed_datasets'\n",
    "if not os.path.exists(save_path_for_data):\n",
    "    os.makedirs(save_path_for_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to save intermediate results\n",
    "def save_text(text, filename):\n",
    "    dump(text, open(save_path_for_data + '/' + filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load intermediate results\n",
    "def load_text(filename):\n",
    "    data = load(open(save_path_for_data + '/' + filename, 'rb'))\n",
    "    print('Loaded: %s' % filename)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean / Normalize the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for cleaning / normalizing the datasets\n",
    "from contractions import contractions_dict\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "# for lemmatization:\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pattern.text.en import tag\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definition for the Normalization process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expand extractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contractions_dict.keys())), flags=re.IGNORECASE|re.DOTALL)\n",
    "    \n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contractions_dict.get(match)\\\n",
    "                                if contractions_dict.get(match)\\\n",
    "                                else contractions_dict.get(match.lower())\\\n",
    "                                    if  contractions_dict.get(match.lower())\\\n",
    "                                    else contractions_dict.get(match.capitalize())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove special characters and symbols (and replace language specific letters such as \"ñ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_characters(text):\n",
    "    # Remove all characters that are not part of the ascii alphabet\n",
    "    text = normalize('NFD', text).encode('ascii', 'ignore')\n",
    "    text = text.decode('UTF-8')\n",
    "    # Remove all unwanted symbols\n",
    "    text = re.sub(r'[_\"\\'\\%()|.,;:+&=*%!?#$@\\[\\]/]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lower_case(text):\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotate text tokens with POS tags\n",
    "def pos_tag_text(text):\n",
    "    \n",
    "    # Translate POS tags to WordNet tags for later usage in the lemmatize function\n",
    "    def translate_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    tagged_text = tag(text)\n",
    "    tagged_lower_text = [(word.lower(), translate_to_wn_tags(pos_tag)) for word, pos_tag in tagged_text]\n",
    "    return tagged_lower_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the input text based on POS tags\n",
    "def lemmatize_text(text):\n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag else word for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all normalization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text, lemmatizeText=False):\n",
    "    normalized_text = []\n",
    "    for sentence in text:\n",
    "        sentence = expand_contractions(sentence)\n",
    "        sentence = remove_special_characters(sentence)\n",
    "        sentence = convert_to_lower_case(sentence)\n",
    "        if lemmatizeText:\n",
    "            sentence = lemmatize_text(sentence)\n",
    "        normalized_text.append(sentence)\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize datasets (apply normalization functions) and save intermediate results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en = normalize_text(text_en, lemmatizeText=False)\n",
    "text_es = normalize_text(text_es, lemmatizeText=False)\n",
    "# NOTE: Runtime for all sentences: ca. 45 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_text(text_en, 'normalized_text_en.pkl')\n",
    "save_text(text_es, 'normalized_text_es.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load normalized datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: normalized_text_en.pkl\n",
      "Loaded: normalized_text_es.pkl\n"
     ]
    }
   ],
   "source": [
    "text_en = load_text('normalized_text_en.pkl')\n",
    "text_es = load_text('normalized_text_es.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization of datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Imports\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for (word) tokenizing the sentences of the datasets\n",
    "def tokenize(text):\n",
    "    tokenized_text = []\n",
    "    for sentence in text:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tokenized_sentence = [token.strip() for token in tokens]\n",
    "        tokenized_text.append(tokenized_sentence)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the datasets\n",
    "text_en = tokenize(text_en)\n",
    "text_es = tokenize(text_es)\n",
    "# NOTE: Runtime for all the data: ca 15-20 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tokenized datasets\n",
    "save_text(text_en, 'tokenized_text_en.pkl')\n",
    "save_text(text_es, 'tokenized_text_es.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load tokenized datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: tokenized_text_en.pkl\n",
      "Loaded: tokenized_text_es.pkl\n"
     ]
    }
   ],
   "source": [
    "text_en = load_text('tokenized_text_en.pkl')\n",
    "text_es = load_text('tokenized_text_es.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove empty sentences\n",
    "\n",
    "*Note*: This step can also be done as part of the normalization process. However, I determined the necessity to remove empty sentences much later which is the reason why the removal of the empty sentences is done after the tokenization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observe datasets for empty sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimize dataset for observation\n",
    "text_en_short = text_en[0:5000]\n",
    "text_es_short = text_es[0:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the indices of the empty sentences\n",
    "def get_empty_sentence_indices(text):\n",
    "    indices = []\n",
    "    i = 0 \n",
    "    for sentence in text:\n",
    "        if len(sentence) == 0:\n",
    "            indices.append(i)\n",
    "        i += 1\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the datasets checking their accordance of empty sentence indices\n",
    "index_en = get_empty_sentence_indices(text_en_short[0:5000])\n",
    "index_es = get_empty_sentence_indices(text_es_short[0:5000])\n",
    "index_both = list(set(index_en).intersection(index_es))\n",
    "\n",
    "print(str(len(index_en)) + ' empty sentences in English dataset: ' + str(index_en))\n",
    "print(str(len(index_es)) + ' empty sentences in Spanish dataset: ' + str(index_es))\n",
    "print('Number of empty sentences in both datasets: ' + str(len(index_both)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse data to investigate the reason for discrepancies in the indices of both datasets\n",
    "for i in range(103,105):\n",
    "    print(text_en[i])\n",
    "for i in range(103,105):\n",
    "    print(text_es[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Consequence of observing the sequence lenghts of the datasets*:<br>\n",
    "From the example above one can see that there are differences in the indices of the empty results. As can been seen aswell these differences result from mistakes in the dataset according to splitting the data. Therefor I consider it necessary to do the following steps to remove all possible mistakable data:\n",
    "- If both datasets have an empty sentence at the same index: Remove the empty sentence\n",
    "- If only one dataset has an empty sentence at a certain index: Remove the sentences at and around the index for both datasets (--> thereby it is assured that no parts of a sentence in one dataset is missing in the other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions to remove empty sentences according to strategy named above**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to define the indices that need to be removed according to strategy\n",
    "def get_indices_for_remove(text_1, text_2):\n",
    "    indices = []\n",
    "    for index in range(len(text_1)):\n",
    "        if len(text_1[index]) == 0 and len(text_2[index]) == 0:\n",
    "            indices.append(index)\n",
    "        elif len(text_1[index]) == 0 or len(text_2[index]) == 0:\n",
    "            indices.append(index-1)\n",
    "            indices.append(index)\n",
    "            indices.append(index+1)\n",
    "        else:\n",
    "            continue\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the empty sentences\n",
    "# Returning datasets where all sentences have a counterpart in the other dataset\n",
    "def remove_empty_sentences_of_texts(text_1, text_2):\n",
    "    indices_for_remove = get_indices_for_remove(text_1, text_2)\n",
    "    clean_text_1 = []\n",
    "    clean_text_2 = []\n",
    "    for index in range(len(text_1)):\n",
    "        if index not in indices_for_remove:\n",
    "            clean_text_1.append(text_1[index])\n",
    "            clean_text_2.append(text_2[index])\n",
    "    \n",
    "    return clean_text_1, clean_text_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove empty sentences from datasets and save intermediate results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en, text_es = remove_empty_sentences_of_texts(text_en, text_es)\n",
    "# Note: Runtime: ca 10 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: preprocessed_text_en.pkl\n",
      "Saved: preprocessed_text_es.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save preprocessed datasets\n",
    "save_text(text_en, 'preprocessed_text_en.pkl')\n",
    "save_text(text_es, 'preprocessed_text_es.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load preprocessed datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en = load_text('preprocessed_text_en.pkl')\n",
    "text_es = load_text('preprocessed_text_es.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create smaller dataset for faster training / experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en_small = text_en[0:100000]\n",
    "text_es_small = text_es[0:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: preprocessed_text_en_small.pkl\n",
      "Saved: preprocessed_text_es_small.pkl\n"
     ]
    }
   ],
   "source": [
    "save_text(text_en_small, 'preprocessed_text_en_small.pkl')\n",
    "save_text(text_es_small, 'preprocessed_text_es_small.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
